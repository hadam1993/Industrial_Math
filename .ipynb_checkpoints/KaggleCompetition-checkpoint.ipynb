{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T15:36:24.033787Z",
     "start_time": "2020-03-11T15:36:24.018121Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "import re\n",
    "warnings.filterwarnings('ignore')\n",
    "from pattern.en import spelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_lengthening(text):\n",
    "    pattern = re.compile(r\"([a-zA-Z])\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_spaces(text):\n",
    "    pattern = re.compile(r\"(\\s)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " asdrfr \n"
     ]
    }
   ],
   "source": [
    "text = \"       asdrfr    \"\n",
    "print(remove_extra_spaces(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lool\n",
      "[('look', 0.7941176470588235), ('fool', 0.07282913165266107), ('wool', 0.058823529411764705), ('pool', 0.036414565826330535), ('cool', 0.015406162464985995), ('tool', 0.00980392156862745), ('loop', 0.0056022408963585435), ('loot', 0.004201680672268907), ('loom', 0.0028011204481792717)]\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import suggest\n",
    "sentence = \"The score has gone finalllllll\"\n",
    "word = \"loool\"\n",
    "word_wlf = reduce_lengthening(word) #calling function defined above\n",
    "print(word_wlf) #word lengthening isn't being able to fix it completely\n",
    "\n",
    "correct_word = suggest(word_wlf) \n",
    "print(correct_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def lemmatize(tweet, nlp):\n",
    "    doc = nlp(tweet)\n",
    "    newSentence = \" \".join([token.lemma_ for token in doc])\n",
    "    return(newSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spellCheck(tweet):\n",
    "    tokenz = word_tokenize(tweet)\n",
    "    newSentence = \"\"\n",
    "    for token in tokenz:\n",
    "        correct_word = suggest(token)\n",
    "        newWord = correct_word[0][0]\n",
    "        newSentence += newWord + \" \"\n",
    "    newSentence = newSentence[:-1]\n",
    "    return(newSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordninja\n",
    "def splitWords(tweet):\n",
    "    doc = nlp(tweet)\n",
    "    newSentence = \" \".join([\" \".join(wordninja.split(str(token))) for token in doc])\n",
    "    return(newSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T15:36:24.271474Z",
     "start_time": "2020-03-11T15:36:24.255839Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T15:36:24.549667Z",
     "start_time": "2020-03-11T15:36:24.487182Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id</td>\n",
       "      <td>keyword</td>\n",
       "      <td>location</td>\n",
       "      <td>text</td>\n",
       "      <td>target</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  keyword  location                                               text  \\\n",
       "0  id  keyword  location                                               text   \n",
       "1   1      NaN       NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "2   4      NaN       NaN             Forest fire near La Ronge Sask. Canada   \n",
       "3   5      NaN       NaN  All residents asked to 'shelter in place' are ...   \n",
       "4   6      NaN       NaN  13,000 people receive #wildfires evacuation or...   \n",
       "\n",
       "   target  \n",
       "0  target  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('./data/Kaggle/train.csv',delimiter=',',\\\n",
    "                           names=['id','keyword','location', 'text','target'])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T15:36:24.803454Z",
     "start_time": "2020-03-11T15:36:24.772218Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text</td>\n",
       "      <td>target</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0                                               text  target\n",
       "1  Our Deeds are the Reason of this #earthquake M...       1\n",
       "2             Forest fire near La Ronge Sask. Canada       1\n",
       "3  All residents asked to 'shelter in place' are ...       1\n",
       "4  13,000 people receive #wildfires evacuation or...       1"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop Id, Keyword, Location\n",
    "dataset = dataset.drop(labels=['id', 'keyword','location'], axis=1)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T15:36:25.088310Z",
     "start_time": "2020-03-11T15:36:25.057042Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "#from string import maketrans\n",
    "def clean(tweet,nlp):\n",
    "    # Special characters\n",
    "    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n",
    "    tweet = re.sub(r\"let\\x89Ûªs\", \"let's\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n",
    "    tweet = re.sub(r\"å_\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n",
    "    tweet = re.sub(r\"åÊ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"åÈ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)    \n",
    "    tweet = re.sub(r\"Ì©\", \"e\", tweet)\n",
    "    tweet = re.sub(r\"å¨\", \"\", tweet)\n",
    "    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n",
    "    tweet = re.sub(r\"åÇ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"å£3million\", \"3 million\", tweet)\n",
    "    tweet = re.sub(r\"åÀ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"amp\", \"and\", tweet)\n",
    "    tweet = re.sub(r\"\\n\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\r\", \"\", tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = reduce_lengthening(tweet)\n",
    "    \n",
    "    tweet = re.sub(r\"x\\d+\", \"\", tweet) \n",
    "    tweet = re.sub(r\"\\d\", \"\", tweet) \n",
    "    tweet = re.sub(r\"\\u0089ã¢\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\s{2,}\", \" \", tweet)\n",
    "    # Remove http\n",
    "    tweet = re.sub(r\"http[^\\s]+\",\"\", tweet)\n",
    "    tweet = re.sub(r\"http\",\"\", tweet)\n",
    "    tweet = re.sub(r\"youtube\",\"\", tweet)\n",
    "    # Remove @abc\n",
    "    tweet = re.sub(r\"@[^\\s]+\", \"\", tweet)\n",
    "    tweet = tweet.translate(str.maketrans('','',string.punctuation))\n",
    "    tweet = lemmatize(tweet,nlp)\n",
    "    tweet = remove_extra_spaces(tweet)\n",
    "    tweet = re.sub(r\"^\\s+\",\"\", tweet)\n",
    "    tweet = splitWords(tweet)\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T15:36:25.986158Z",
     "start_time": "2020-03-11T15:36:25.717663Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop first row\n",
    "dataset = dataset.drop(index=0)\n",
    "# Clean data\n",
    "nlp = spacy.load('en_core_web_sm',disable=['parser', 'ner'])\n",
    "dataset['text_cleaned'] = dataset['text'].apply(lambda s : clean(s,nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T15:36:30.138694Z",
     "start_time": "2020-03-11T15:36:30.107654Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>PRON deed be the reason of this earthquake may...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la rong e s ask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all resident ask to shelter in place be be not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfire evacuation order in ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just get send this photo from ruby alaska as s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text target  \\\n",
       "1  Our Deeds are the Reason of this #earthquake M...      1   \n",
       "2             Forest fire near La Ronge Sask. Canada      1   \n",
       "3  All residents asked to 'shelter in place' are ...      1   \n",
       "4  13,000 people receive #wildfires evacuation or...      1   \n",
       "5  Just got sent this photo from Ruby #Alaska as ...      1   \n",
       "\n",
       "                                        text_cleaned  \n",
       "1  PRON deed be the reason of this earthquake may...  \n",
       "2            forest fire near la rong e s ask canada  \n",
       "3  all resident ask to shelter in place be be not...  \n",
       "4  people receive wildfire evacuation order in ca...  \n",
       "5  just get send this photo from ruby alaska as s...  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['text_cleaned'] = dataset['text_cleaned'].drop_duplicates()\n",
    "dataset['text_cleaned'].replace('', np.nan, inplace=True)\n",
    "dataset.dropna(subset=['text_cleaned'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>PRON deed be the reason of this earthquake may...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la rong e s ask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all resident ask to shelter in place be be not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfire evacuation order in ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just get send this photo from ruby alaska as s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text target  \\\n",
       "1  Our Deeds are the Reason of this #earthquake M...      1   \n",
       "2             Forest fire near La Ronge Sask. Canada      1   \n",
       "3  All residents asked to 'shelter in place' are ...      1   \n",
       "4  13,000 people receive #wildfires evacuation or...      1   \n",
       "5  Just got sent this photo from Ruby #Alaska as ...      1   \n",
       "\n",
       "                                        text_cleaned  \n",
       "1  PRON deed be the reason of this earthquake may...  \n",
       "2            forest fire near la rong e s ask canada  \n",
       "3  all resident ask to shelter in place be be not...  \n",
       "4  people receive wildfire evacuation order in ca...  \n",
       "5  just get send this photo from ruby alaska as s...  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T15:36:30.639226Z",
     "start_time": "2020-03-11T15:36:30.569988Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset['text_cleaned'].to_csv(\"dataset_cleaned2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "dataset['tokenized'] = dataset['text_cleaned'].apply(tt.tokenize)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordList = sum(dataset['tokenized'].values,[])\n",
    "wordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freDist = nltk.FreqDist(wordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freDist.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Tokenized BERT Embeding and Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_token_list(sample):\n",
    "    # Find the sentence with the max length\n",
    "    max_len = 0\n",
    "    for token_list in sample:\n",
    "        if len(token_list) > max_len:\n",
    "            max_len = len(token_list)\n",
    "    # Adjust every sentence to the same length\n",
    "    padded = np.array([token_list + [0]*(max_len-len(token_list)) for token_list in sample])\n",
    "    return padded, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_from_sample(sample, model):\n",
    "    # Pad sample data:\n",
    "#     sample = pad_token_list(sample)\n",
    "    # Define mask from data: - 0 token entry     -> padding, set mask entry to 0\n",
    "    #                        - non-0 token entry -> valid word, set mask entry to 1\n",
    "    mask = np.where(sample != 0, 1, 0)\n",
    "    \n",
    "    # Create tensor objects from numpy arrays\n",
    "    input_ids = torch.tensor(sample).long()\n",
    "    attention_mask = torch.tensor(mask).long()\n",
    "\n",
    "    # Use BERT model to get embeddings\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "    # Extract [CLS] embedding for each sample as numpy array to be used for classification task\n",
    "    features = last_hidden_states[0][:,0,:].numpy()\n",
    "    return features, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 4000\n",
    "random_sample = dataset.sample(n=sample_size)\n",
    "random_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_set = dataset.loc[dataset.index.difference(random_sample.index)]\n",
    "val_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_random_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sample_size = 1500\n",
    "val_random_sample = val_set.sample(n=val_sample_size)\n",
    "val_random_sample.shape\n",
    "test_set = val_set.loc[val_set.index.difference(val_random_sample.index)]\n",
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For DistilBERT:\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_tokenized = random_sample['text_cleaned'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "#sample_tokenized2 = random_sample2['text_cleaned'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "val_random_sample_tokenized = val_random_sample['text_cleaned'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "#test_tokenized = test_set['text_cleaned'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_padded, sample_len = pad_token_list(sample_tokenized.values)\n",
    "val_padded, val_len = pad_token_list(val_random_sample_tokenized.values)\n",
    "#sample_padded2, sample_len2 = pad_token_list(sample_tokenized2.values)\n",
    "#test_padded, test_len = pad_token_list(test_tokenized.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_features, mask = get_embeddings_from_sample(sample_padded, model)\n",
    "val_features, mask = get_embeddings_from_sample(val_padded, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cuda device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_tensor = torch.tensor(np.asarray(sample_features))\n",
    "train_features_tensor = train_features_tensor.to(device)\n",
    "train_labels_tensor =  torch.tensor(np.asarray(random_sample['target']).astype(np.int))\n",
    "train_labels_tensor = train_labels_tensor.to(device)\n",
    "\n",
    "val_features_tensor = torch.tensor(np.asarray(val_features))\n",
    "val_features_tensor = val_features_tensor.to(device)\n",
    "val_labels_tensor =  torch.tensor(np.asarray(val_random_sample['target']).astype(np.int))\n",
    "val_labels_tensor = val_labels_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neural network class to be trained\n",
    "# Structure:\n",
    "# input -> fc1 -> sigmoid -> out -> log_softmax\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Shallow_Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Shallow_Network,self).__init__()\n",
    "        self.fc1 = nn.Linear(768,1000)\n",
    "        self.out = nn.Linear(1000,2)\n",
    "    def forward(self,input):\n",
    "        # Take input, feed through fc1 layer,\n",
    "        # then apply activation function to it\n",
    "        x = F.sigmoid(self.fc1(input))\n",
    "        # Take output of sigmoid, input into out layer,\n",
    "        # and apply log_softmax function\n",
    "        return (F.log_softmax(self.out(x),dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create neural network object\n",
    "net = Shallow_Network()\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "#Create an stochastic gradient descent optimizer\n",
    "adam = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_func = nn.NLLLoss()\n",
    "loss_func = loss_func.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(net,features,labels):\n",
    "    # Get classification probabilities from hidden state array\n",
    "    # And apply Softmax\n",
    "    with torch.no_grad():\n",
    "        probs = net(features)\n",
    "        softprobs = F.softmax(probs)\n",
    "    # Get most likely class and its index for each sample point\n",
    "    values, indices = torch.max(softprobs,1)\n",
    "    # Calculate number of sample points where prediction failed\n",
    "    nums = torch.sum(torch.abs(labels-indices)).detach().cpu().numpy()\n",
    "    # Number of correct predictions\n",
    "    numcorrect = len(labels)-(nums+0)\n",
    "    # Accuracy of prediction\n",
    "    accuracy = numcorrect/len(labels)\n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train network\n",
    "cnt = 0\n",
    "average_losses = []\n",
    "average_val_losses = []\n",
    "acc = []\n",
    "cur_loss = []\n",
    "min_validation = 10000.0\n",
    "min_val_epoch = 0\n",
    "for epoch in range(400):\n",
    "    net.train()\n",
    "    #zero the gradient\n",
    "    adam.zero_grad()\n",
    "    #Get output of network\n",
    "    probs = net(train_features_tensor)\n",
    "    #compute loss\n",
    "    loss = loss_func(probs,train_labels_tensor)\n",
    "    #compute the backward gradient and move network in that direction\n",
    "    loss.backward()\n",
    "    adam.step()\n",
    "    #gather loss\n",
    "    cur_loss.append(loss.detach().cpu().numpy())\n",
    "    print(\"epoch \",epoch)\n",
    "    print(\"training loss: \", np.mean(cur_loss))\n",
    "    net.eval()\n",
    "    probs_val = net(val_features_tensor)\n",
    "    loss_val = loss_func(probs_val,val_labels_tensor)\n",
    "    print(\"validation loss: \", np.mean(loss_val.detach().cpu().numpy()))\n",
    "    print(\"validation accuracy: \", accuracy(net,val_features_tensor,val_labels_tensor))\n",
    "    #Save model if validation is min\n",
    "    if min_validation > np.mean(loss_val.detach().cpu().numpy()):\n",
    "        min_validation = np.mean(loss_val.detach().cpu().numpy())\n",
    "        min_val_epoch = epoch\n",
    "        torch.save(net.state_dict(), './net_parameters_kaggle.pth')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Shallow_Network()\n",
    "checkpoint = torch.load('./net_parameters_kaggle.pth')\n",
    "net.load_state_dict(checkpoint)\n",
    "net = net.to(device)\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_val = net(val_features_tensor)\n",
    "loss_val = loss_func(probs_val,val_labels_tensor)\n",
    "print(\"validation loss: \", np.mean(loss_val.detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy(net,val_features_tensor,val_labels_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = val_set.loc[val_set.index.difference(val_random_sample.index)]\n",
    "test_random_sample_tokenized = test_set['text_cleaned'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "test_padded, test_len = pad_token_list(test_random_sample_tokenized.values)\n",
    "test_features, mask = get_embeddings_from_sample(test_padded, model)\n",
    "test_features_tensor = torch.tensor(np.asarray(test_features))\n",
    "test_features_tensor = test_features_tensor.to(device)\n",
    "test_labels_tensor =  torch.tensor(np.asarray(test_set['target']).astype(np.int))\n",
    "test_labels_tensor = test_labels_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy(net,test_features_tensor,test_labels_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline only html tag and special character cleaning yielded 81.8% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T15:36:39.547585Z",
     "start_time": "2020-03-11T15:36:39.532169Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T15:36:39.962932Z",
     "start_time": "2020-03-11T15:36:39.932119Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset.drop_duplicates(subset='text_cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T15:36:48.645628Z",
     "start_time": "2020-03-11T15:36:48.630451Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6957, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T15:36:49.469026Z",
     "start_time": "2020-03-11T15:36:49.431710Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text target  \\\n",
       "1  Our Deeds are the Reason of this #earthquake M...      1   \n",
       "2             Forest fire near La Ronge Sask. Canada      1   \n",
       "3  All residents asked to 'shelter in place' are ...      1   \n",
       "4  13,000 people receive #wildfires evacuation or...      1   \n",
       "5  Just got sent this photo from Ruby #Alaska as ...      1   \n",
       "\n",
       "                                        text_cleaned  \n",
       "1  Our Deeds are the Reason of this #earthquake M...  \n",
       "2             Forest fire near La Ronge Sask. Canada  \n",
       "3  All residents asked to 'shelter in place' are ...  \n",
       "4  13,000 people receive #wildfires evacuation or...  \n",
       "5  Just got sent this photo from Ruby #Alaska as ...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ã¢\n",
    "&gt;\n",
    "&amp;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
